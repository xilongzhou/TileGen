{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('objectInfo150.txt', sep='\\t', lineterminator='\\n')\n",
    "name2idx = {}\n",
    "for i in range(150):\n",
    "    line = data.loc[i]\n",
    "    name2idx[ line['Name']  ] = line['Idx']\n",
    "\n",
    "   \n",
    "   \n",
    "class Opt_train:\n",
    "    train = True\n",
    "    aug = True\n",
    "   \n",
    "    full_data_dir = '../../data/ade20k/full_data_bedroom/'\n",
    "   \n",
    "    bg_size=512\n",
    "   \n",
    "    fg_img_size=256\n",
    "    fg_seg_size=256\n",
    "    fg_sem_size=128\n",
    "\n",
    "   \n",
    "   \n",
    "class Opt_test:\n",
    "    train = False\n",
    "    aug = False  \n",
    "   \n",
    "    full_data_dir = '../../data/ade20k/full_data_bedroom/'\n",
    "   \n",
    "    bg_size = 512\n",
    "   \n",
    "    fg_img_size=256\n",
    "    fg_seg_size=256\n",
    "    fg_sem_size=128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "\n",
    "def get_box(mask):\n",
    "    \"mask should be a 2D np.array \"\n",
    "    if mask.sum()==0:\n",
    "        return 0,0,0,0 # means this object is cropped out during aug\n",
    "    y,x = np.where(mask == 1)\n",
    "    x1,x2,y1,y2 = x.min(),x.max(),y.min(),y.max()\n",
    "    w = x2-x1\n",
    "    h = y2-y1\n",
    "    return x1,y1,x2,y2\n",
    "\n",
    "\n",
    "def enlarge_box(x1, y1, x2, y2, width, height, ratio):\n",
    "    w, h = x2-x1, y2-y1\n",
    "    r = int( max(w,h) * (ratio/2) )\n",
    "    center_x = int( (x1+x2)/2 )\n",
    "    center_y = int( (y1+y2)/2 )\n",
    "    y1 = max(0, center_y-r)\n",
    "    y2 = min(height, center_y+r)\n",
    "    x1 = max(0, center_x-r)\n",
    "    x2 = min(width, center_x+r)\n",
    "    return x1, y1, x2, y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, fg_classes, train):\n",
    "       \n",
    "        opt = Opt_train if train else Opt_test\n",
    "       \n",
    "        self.train = train  \n",
    "        self.full_data_dir = opt.full_data_dir  \n",
    "        self.aug = opt.aug\n",
    "       \n",
    "        self.bg_size = opt.bg_size\n",
    "        self.fg_img_size = opt.fg_img_size\n",
    "        self.fg_sem_size = opt.fg_sem_size\n",
    "        self.fg_seg_size = opt.fg_seg_size\n",
    "       \n",
    "       \n",
    "        temp = 'training' if self.train else 'validation'\n",
    "       \n",
    " \n",
    "        self.img_files = os.listdir( os.path.join( self.full_data_dir,'images', temp ) )\n",
    "        self.sem_files = os.listdir( os.path.join( self.full_data_dir,'annotations', temp ) )  \n",
    "        self.ins_files = os.listdir( os.path.join( self.full_data_dir,'annotations_instance', temp ) )\n",
    "        assert( len(self.img_files)==len(self.sem_files)==len(self.ins_files)    )\n",
    "       \n",
    "        self.img_files = [  os.path.join( self.full_data_dir,'images', temp, item ) for item in self.img_files ]\n",
    "        self.sem_files = [  os.path.join( self.full_data_dir,'annotations', temp, item ) for item in self.sem_files ]\n",
    "        self.ins_files = [  os.path.join( self.full_data_dir,'annotations_instance', temp, item ) for item in self.ins_files ]\n",
    "       \n",
    "        self.img_files.sort()\n",
    "        self.sem_files.sort()\n",
    "        self.ins_files.sort()\n",
    "               \n",
    "       \n",
    "        self.fg_classes = fg_classes  \n",
    "       \n",
    "        for name in fg_classes:\n",
    "            assert name in name2idx\n",
    "       \n",
    "\n",
    "        with open(  self.full_data_dir+'ins_of_each_sem_'+temp+'.txt', \"rb\") as fp:  \n",
    "            self.ins_of_each_sem = pickle.load(fp)\n",
    "           \n",
    "           \n",
    "    def exist_check(self, x1, y1, x2, y2, W, H):\n",
    "       \n",
    "        # these are box location of this instance in final fg image\n",
    "        new_x1 = int(self.bg_size/W *x1)\n",
    "        new_x2 = int(self.bg_size/W *x2)\n",
    "        new_y1 = int(self.bg_size/H *y1)\n",
    "        new_y2 = int(self.bg_size/H *y2)\n",
    "        if new_x1>=new_x2 or new_y1>=new_y2:\n",
    "            return False, None  # means this object is too small to exist in final scene\n",
    "\n",
    "       \n",
    "        return True, { 'new_box':[new_x1,new_y1,new_x2,new_y2],  'new_size':[new_y2-new_y1, new_x2-new_x1]  }\n",
    "\n",
    "   \n",
    "   \n",
    "    def instance_process(self, img, sem, ins, ins_idxs):\n",
    "        \"\"\"\n",
    "        This is a processer for each instance\n",
    "        img, sem and ins are all in original resolution,\n",
    "        ins_idxs is a list contains ins idxes wanted for this class\n",
    "        It will return a list contaning multiple dict\n",
    "        and each dict has information for each instance\n",
    "        \"\"\"\n",
    "        W,H =img.size\n",
    "       \n",
    "        results = []\n",
    "        ins_array = np.array(ins)\n",
    "       \n",
    "        for idx in ins_idxs:\n",
    "           \n",
    "            # get box for this instance\n",
    "            this_instance_mask = (ins_array==idx)    \n",
    "            x1, y1, x2, y2 = get_box(this_instance_mask)\n",
    "           \n",
    "            # check if this instance will be presented in final scene if so its box and size\n",
    "            exist, result = self.exist_check(x1, y1, x2, y2, W, H)\n",
    "           \n",
    "            if exist:                \n",
    "                # crop img and ins mask (name it seg)\n",
    "                img = img.crop([x1, y1, x2, y2]).resize( (self.fg_img_size,self.fg_img_size), Image.NEAREST )          \n",
    "                seg = ins.crop([x1, y1, x2, y2]).resize( (self.fg_seg_size,self.fg_seg_size), Image.NEAREST )\n",
    "               \n",
    "                # enlarge current box to give more global information and crop sem\n",
    "                x1, y1, x2, y2 = enlarge_box(x1, y1, x2, y2, W, H, 2) # hardcoded enlarge twice\n",
    "                sem = sem.crop([x1, y1, x2, y2]).resize( (self.fg_sem_size,self.fg_sem_size), Image.NEAREST  )\n",
    "               \n",
    "                #transform then into tensor\n",
    "                result['img'] = ((TF.to_tensor(img)-0.5)/0.5).unsqueeze(0)    \n",
    "                result['sem'] = torch.tensor( np.array(sem) ).unsqueeze(0).unsqueeze(0).long()\n",
    "                result['seg'] = torch.tensor(  (np.array(seg)==idx)*1  ).unsqueeze(0).unsqueeze(0).float()  \n",
    "       \n",
    "                results.append(result)\n",
    "           \n",
    "        return results\n",
    "       \n",
    "       \n",
    "       \n",
    "               \n",
    "    def main_process(self, img, sem, ins, ins_sem):\n",
    "           \n",
    "        # get img and sem with resolution used in bg\n",
    "        bg_img = img.resize( (self.bg_size,self.bg_size), Image.NEAREST )  \n",
    "        bg_sem = sem.resize( (self.bg_size,self.bg_size), Image.NEAREST )  # used both in bg and final spade\n",
    "         \n",
    "\n",
    "        # create fg instance each time  \n",
    "        fg_data = {}\n",
    " \n",
    "        for class_name in self.fg_classes:    \n",
    "       \n",
    "            if class_name in ins_sem: # it means this image has this semantic\n",
    "                this_class_fg_data = self.instance_process(img, sem, ins, ins_sem[class_name] )\n",
    "               \n",
    "                if len(this_class_fg_data) != 0:   #otherwise means all instance of this sem are cropped out\n",
    "                    fg_data[class_name] = this_class_fg_data\n",
    "       \n",
    "       \n",
    "        bg_img = ((TF.to_tensor(bg_img)-0.5)/0.5).unsqueeze(0)\n",
    "        bg_sem = torch.tensor( np.array(bg_sem) ).unsqueeze(0).unsqueeze(0).long()  \n",
    "        bg_data = {   'bg_img':bg_img, 'bg_sem':bg_sem   }\n",
    "         \n",
    "        return bg_data, fg_data\n",
    "\n",
    "\n",
    "   \n",
    "    def transform(self, img, sem, ins):\n",
    "        if not self.aug:\n",
    "            return img, sem, ins\n",
    "\n",
    "        if random.random() > 0.5:      \n",
    "            img = TF.hflip(img)\n",
    "            sem = TF.hflip(sem)\n",
    "            ins = TF.hflip(ins)\n",
    "           \n",
    "           \n",
    "        W,H = img.size\n",
    "        new_w, new_h = int(W*random.uniform(0.8, 1)), int(H*random.uniform(0.8, 1))\n",
    "\n",
    "        # Random crop\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(img, output_size=(new_h, new_w))\n",
    "        img = TF.crop(img, i, j, h, w)\n",
    "        sem = TF.crop(sem, i, j, h, w)\n",
    "        ins = TF.crop(ins, i, j, h, w)\n",
    "       \n",
    "        return img, sem, ins\n",
    "\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "       \n",
    "        # real full image, semantic map, instance map      \n",
    "        img = Image.open( self.img_files[idx]  )\n",
    "        sem = Image.open( self.sem_files[idx]  )\n",
    "        ins = Image.open( self.ins_files[idx]  )\n",
    "       \n",
    "        # read ins info for each sem\n",
    "        ins_sem = self.ins_of_each_sem[idx]\n",
    "       \n",
    "        # apply data aug if specified\n",
    "        img, sem, ins = self.transform(img, sem, ins)\n",
    "   \n",
    "        # process all data\n",
    "        bg_data, fg_data = self.main_process(img, sem, ins, ins_sem)\n",
    "     \n",
    "        return bg_data, fg_data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader(fg_classes, train, batch_size=32, shuffle=True, drop_last=True):\n",
    "    \"\"\"\n",
    "    DONT SET NUMBER OF WORKERS ON V8, SOME MULTIPLE PROCESSING BUGS\n",
    "    WHAT IS IN HERE DOES NOT HELPFUL:\n",
    "    https://github.com/pytorch/pytorch/issues/973\n",
    "    \"\"\"\n",
    "   \n",
    "    def collate_fn(data):\n",
    "        return data\n",
    "\n",
    "    dataset = Dataset(fg_classes, train)\n",
    "    dataloader = DataLoader( dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle, drop_last=drop_last  )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
